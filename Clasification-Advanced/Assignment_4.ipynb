{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today we are going to perform the simple classification of the amazon reviews' sentiment.\n",
    "\n",
    "### Please, download the dataset amazon_baby.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                name  \\\n0                           Planetwise Flannel Wipes   \n1                              Planetwise Wipe Pouch   \n2                Annas Dream Full Quilt with 2 Shams   \n3  Stop Pacifier Sucking without tears with Thumb...   \n4  Stop Pacifier Sucking without tears with Thumb...   \n\n                                              review  rating  \n0  These flannel wipes are OK, but in my opinion ...       3  \n1  it came early and was not disappointed. i love...       5  \n2  Very soft and comfortable and warmer than it l...       5  \n3  This is a product well worth the purchase.  I ...       5  \n4  All of my kids have cried non-stop when I trie...       5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>review</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Planetwise Flannel Wipes</td>\n      <td>These flannel wipes are OK, but in my opinion ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Planetwise Wipe Pouch</td>\n      <td>it came early and was not disappointed. i love...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Annas Dream Full Quilt with 2 Shams</td>\n      <td>Very soft and comfortable and warmer than it l...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n      <td>This is a product well worth the purchase.  I ...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n      <td>All of my kids have cried non-stop when I trie...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "baby_df = pd.read_csv('amazon_baby.csv')\n",
    "baby_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (data preparation)\n",
    "a) Remove punctuation from reviews using the given function.   \n",
    "b) Replace all missing (nan) revies with empty \"\" string.  \n",
    "c) Drop all the entries with rating = 3, as they have neutral sentiment.   \n",
    "d) Set all positive ($\\geq$4) ratings to 1 and negative($\\leq$2) to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "baby_df['review']=baby_df['review'].astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b)\n",
    "baby_df[\"review\"].replace(np.nan, \"\", inplace=True)\n",
    "#short test:\n",
    "baby_df[\"review\"][38] == baby_df[\"review\"][38]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initially, in the process of eliminating all NaN data, we employ a function called 'replace' to substitute one sentence with another. In this function, we replace any instances of NaN with an empty space."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a)\n",
    "baby_df[\"review\"] = baby_df[\"review\"].apply(remove_punctuation)\n",
    "\n",
    "#short test:\n",
    "baby_df[\"review\"][4] == 'All of my kids have cried nonstop when I tried to ween them off their pacifier until I found Thumbuddy To Loves Binky Fairy Puppet  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from itThis is a must buy book and a great gift for expecting parents  You will save them soo many headachesThanks for this book  You all rock'\n",
    "remove_punctuation(baby_df[\"review\"][4]) == 'All of my kids have cried nonstop when I tried to ween them off their pacifier until I found Thumbuddy To Loves Binky Fairy Puppet  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from itThis is a must buy book and a great gift for expecting parents  You will save them soo many headachesThanks for this book  You all rock'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Subsequently, following the conversion of all reviews to text format, we utilize a provided function that eliminates all punctuation marks. This is accomplished by employing the ‘apply’ option provided by the pandas package."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c)\n",
    "a = 0\n",
    "for x in baby_df[\"rating\"]:\n",
    "\n",
    "    if x == 3:\n",
    "        baby_df.drop(index = a, inplace = True, axis=0)\n",
    "        a=a+1\n",
    "    else:\n",
    "        a=a+1\n",
    "        continue\n",
    "\n",
    "#short test:\n",
    "sum(baby_df[\"rating\"] == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moving forward, the subsequent task involves eliminating the lines with a rating of 3. This is achieved by searching through the file for these specific lines and then removing them using the standard for loop in conjunction with the drop function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d) \n",
    "baby_df[\"rating\"].replace(1,-1, inplace = True)\n",
    "baby_df[\"rating\"].replace(4,1, inplace = True)\n",
    "baby_df[\"rating\"].replace(5,1, inplace = True)\n",
    "baby_df[\"rating\"].replace(2,-1, inplace = True)\n",
    "#short test:\n",
    "sum(baby_df[\"rating\"]**2 != 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upon completing this task, the final step entails converting all scores above 3 to 1 and those below 3 to -1. To accomplish this, we utilize the 'replace' function as employed previously, focusing on substituting numerical values this time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "In order to analyze strings, we need to assign them numerical values. We will use one of the simplest string representation, which transforms strings into the $n$ dimensional vectors. The number of dimensions will be the size of our dictionary, and then the values of the vector will represent the number of appereances of the given word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 1 0 1 0 1]\n",
      " [1 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 2 1 0 1]\n",
      " [0 0 0 1 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "reviews_train_example = [\"We like apples\",\n",
    "                   \"We hate oranges\",\n",
    "                   \"I adore bananas\",\n",
    "                   \"We like like apples and oranges\",\n",
    "                   \"They dislike bananas\"]\n",
    "\n",
    "X_train_example = vectorizer.fit_transform(reviews_train_example)\n",
    "\n",
    "# print(vectorizer.get_feature_names())\n",
    "print(X_train_example.todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 1 0 1 0]\n",
      " [0 1 1 1 0 1 0 1 0 1]\n",
      " [0 0 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "reviews_test_example = [\"They like bananas\",\n",
    "                   \"We hate oranges bananas and apples\",\n",
    "                   \"We love bananas\"] #New word!\n",
    "\n",
    "X_test_example = vectorizer.transform(reviews_test_example)\n",
    "\n",
    "print(X_test_example.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should acknowledge few facts. Firstly, CountVectorizer does not take order into account. Secondly, it ignores one-letter words (this can be changed during initialization). Finally, for test values, CountVectorizer ignores words which are not in it's dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 \n",
    "a) Split dataset into training and test sets.     \n",
    "b) Transform reviews into vectors using CountVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#a)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(baby_df, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#b)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x = vectorizer.fit_transform(list(train[\"review\"]))\n",
    "y = train[\"rating\"]\n",
    "x_test = vectorizer.transform(list(test[\"review\"]))\n",
    "y_test = test[\"rating\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the subsequent task, we partition the data into training and test sets. Following that, we convert the words present in the reviews into arrays of numbers (vectors). This process enables us to instruct the program in recognizing specific characteristics of words through numerical representations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 \n",
    "a) Train LogisticRegression model on training data (reviews processed with CountVectorizer, ratings as they were).   \n",
    "b) Print 10 most positive and 10 most negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\PycharmProjects\\Data-Science\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=200)",
      "text/html": "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=200)</pre></div></div></div></div></div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most positive words:  ['worry' 'sooner' 'con' 'wonderfully' 'downside' 'saves' 'minor'\n",
      " 'skeptical' 'lifesaver' 'penny']\n",
      "most negative words:  ['dissapointed' 'worthless' 'theory' 'unusable' 'nope' 'worst' 'poorly'\n",
      " 'disappointing' 'listened' 'shame']\n"
     ]
    }
   ],
   "source": [
    "#b)\n",
    "zipped_coef = list(zip(list(range(model.coef_.shape[1])), model.coef_[0]))\n",
    "sorted_coef = sorted(zipped_coef, key= lambda v: v[1])\n",
    "sorted_coef = np.array(sorted_coef)\n",
    "sorted_coef_indexes = sorted_coef[:, 0].astype(int)\n",
    "words = np.array(vectorizer.get_feature_names_out())\n",
    "most_positive_words = words[sorted_coef_indexes[-10:]]\n",
    "most_negative_words = words[sorted_coef_indexes[:10]]\n",
    "print(\"most positive words: \", most_positive_words)\n",
    "print(\"most negative words: \", most_negative_words)\n",
    "\n",
    "#hint: model.coef_, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Having previously partitioned the data, we now proceed to fit it to the suitable regression curve known as logistic regression. This enables us to instruct the program on discerning negative and positive words based on their occurrences in the numerical table. The computer is trained to associate a rating of -1 with more negative expressions in the reviews, saving this information for future recognition."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 \n",
    "a) Predict the sentiment of test data reviews.   \n",
    "b) Predict the sentiment of test data reviews in terms of probability.   \n",
    "c) Find five most positive and most negative reviews.   \n",
    "d) Calculate the accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1, -1,  1, ...,  1, -1,  1], dtype=int64)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a)\n",
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.12343292e-03, 9.98876567e-01],\n       [6.45599703e-01, 3.54400297e-01],\n       [2.13849825e-04, 9.99786150e-01],\n       ...,\n       [6.31209931e-03, 9.93687901e-01],\n       [9.99999657e-01, 3.43307809e-07],\n       [4.50613157e-03, 9.95493868e-01]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b)\n",
    "probabilty = model.predict_proba(x_test)\n",
    "probabilty\n",
    "\n",
    "#hint: model.predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most positive:  I bought this carrier when my daughter was about 4 weeks old shes now 10 weeks old  I had a Moby that I borrowed from a friend but could never quite get to work and my daughter hated being in it  I also have a Bjorn Active but she seemed pretty precarious in that when she was so littleThis carrier is nearly perfect for infants  Its not quite as easy to put on as the Bjorn but MUCH easier than the Moby and it gives that nice snug fit that the Moby did  Its much lighter weight than the Bjorn so its easier on my back  I have the khakicolored version so I havent had any problems with it showing dirt and dust  I wish the straps werent so long one size fits all but I just wrap them around to my back and tie them again loosely  Im 54 and 145 lbs  I think this carrier would fit most people  The fabric is thick and the construction seems to be of good quality It might be hot in hot weather but most carriers areThis has been a life saver  My daughter wont sleep anywhere except while being held  Now once shes feddiaperedetc and getting fussy ie needs to sleep I pop her in and walk around and sing  After a couple of minutes of fussing she is sound asleep sometimes for several hours  I can do a little light work around the house plus I can sit and type at the computer shes in the wrap right nowUpdate 812010my daughter is now almost 5 months old and this carrier still works great with her facing me for her naps  She now weighs about 15 lbs and its not bad for a couple hours of use especially if I can sit for part of the time  She starts to strain my back when I have to walk around with her for a couple of hours but less so than when she is forwardfacing in the Bjorn ActiveI have tried the faceout option and it works OK but I probable prefer to face her out in the Bjorn Active its quicker to put on  The Wrap and Tie is a little tall for her facing out so I have to try to scrunch down the front and I worry about her falling asleep and having her nose down in it  This carrier is definitely better than the Bjorn Active for facingin naps more support on the back of her neck so that her head doesnt flop around while shes sleeping  A friend is sending me her Ergo so I will compare all three once Ive tried itUpdate 8252010Once I got the Ergo I stopped using the Wrap and Tie  Both hold the baby snug and close to me facing in for sleeping  The Ergo is super easy to put on and great for my back  I am now an Ergo convertIn summary1 Infantino Wrap and Tie  Pros  Great for little ones facing in for sleeping  Easier to put on than the Moby and yet keeps baby nice and snug  Can face baby out  Sturdy  Cheap cheaper than other mei tai style carriers  Not bad on my back  Can ball it up and shove it in the diaper bag  Cons  harder to put on than BjornErgo not as good as the Bjorn for facing out not as good on the back as the Ergo2 Baby Bjorn I have the Bjorn Active Pros  My baby LOVES facing out when awake and this is handsdown the best carrier for that  I put up with back pain to keep my baby happy  Cons Not as good as the Wrap and Tie or Ergo for facing insleeping  Hurts my back to wear it for long baby now 15 lbs  Expensive3 Ergo Pros  Quick and easy to put on perfect for baby facing insleeping cute little hood to put over her head to block out light while sleeping AWESOME for my back  Cons  Expensive baby cant face out  My baby doesnt much like facing in while awake but shes getting used to itWhen I go out I usually take both the Ergo and the Bjorn with me in the car and then pick whichever one is appropriate for my baby at the moment  If you have enough cash to drop on both or better yet can get them used or handmedown get both and skip the Wrap and Tie  If you only have enough  for one carrier the Wrap and Tie might workfinal updateothers have commented on the babys face being smushed into their chest in the facingin position  I havent had a problem with this but I frequently check my babys neck position to make sure that her chin is tilted up a bit and her nose is pointing up a bit  Its easy to look down at her and reposition her head when appropriate  Also I usually have her in with the binky in her mouth which keeps her face off my chest as well\n",
      "\n",
      " most negative:  This product should be in the hall of fame solely based on its ability to achieve such a high number of positive ratings from parents hoping to stimulate their childs intelligence Dont be fooled its a piece of junk I had such high hopes for this and even at a 15 purchase price I feel ripped offProsNice images with high color contrastCons Simply horrible construction The pieces of plastic could not be more cheap and one of the nubs cracked right off during assembly The arches and misc plastic pieces probably cost all of 20 to manufacture in total The image cards do not easily fit into the attachment nubs and do not hang straight as a result Maybe this changes over time but right now from an underneath view you basically see a card slanted sideways Not interesting IMO The bar that attaches to the side of the crib is a terrible design and it is LAVENDER Seriously it is not white at all It clashes completely with the rest of the product contrary to the image on Amazon and the product boxIts just cheap and hideous really The idea is good but they should be ashamed at the execution The cheap thin plastic and nonmatching parts should be an embarrassment to any manufacturer who claims to me making toys to stimulate a childs thinking and intelligence There was clearly neither thinking or intelligence behind this product\n"
     ]
    }
   ],
   "source": [
    "#c)\n",
    "print(\"most positive: \", test.iloc[probabilty.T[1].argmax()][\"review\"])\n",
    "print(\"\\n most negative: \", test.iloc[probabilty.T[0].argmax()][\"review\"])\n",
    "\n",
    "#hint: use the results of b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9303769002428712"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d) \n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building upon the actions undertaken in task 4, we proceed to identify the opinion containing the highest number of positive words and, conversely, the one with the most negative words. Ultimately, we assess the accuracy of our model, and the results indicate a high level of effectiveness, reaching an accuracy rate of 93%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "In this exercise we will limit the dictionary of CountVectorizer to the set of significant words, defined below.\n",
    "\n",
    "\n",
    "a) Redo exercises 2-5 using limited dictionary.   \n",
    "b) Check the impact of all the words from the dictionary.   \n",
    "c) Compare accuracy of predictions and the time of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "significant_words = ['love','great','easy','old','little','perfect','loves','well','able','car','broke','less','even','waste','disappointed','work','product','money','would','return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most positive:  Check and recheck the KTan for size issues before letting your guard down and wearing NO ISSUES OCCURED however I thought I was good to go and then realized that this large was different than the large I had been given before it Odd but fabric This makes the moby wrap appear like a midevil torture devise and bonus you wont have to worry about some BabyWearing freak watching you in the parking lot at the mall as you struggle to turn a bolt of fabric into what the KTan has simplifiedseriously Dad approved and if the moby is your thing thats cool just perhaps realize that its not as easy as everyone thinks to wrap that amount of fabric around yourself evenly snug comfortable and without mummifying yourself and lord help you your baby  Another bonus to the KTAN issue with baby takes only a second to move her out of the KTAN but in a moby We thought for the first two days after returning the moby 34have you seen Katy34love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it love it\n",
      "\n",
      "most negative:  I really like this shopping cart bag EXCEPT that the zipper that holds this bag together when not in use broke after 2 uses  Since the 2nd use happened after the end of the 30 day return policy AND since I had already used the product of course I did  how else would it break the seller would not accept a return  BabyAge was the seller I purchased through on Amazon and I DO NOT RECOMMEND THEM  The product of course also was at fault so I also contacted BuggyBagg and after a couple voicemails  emails to them I did not receive a return call or return email\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8675302089892357"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a)\n",
    "vectorizer_small = CountVectorizer(vocabulary=significant_words)\n",
    "\n",
    "x_small = vectorizer_small.fit_transform(list(train[\"review\"]))\n",
    "y_small = train[\"rating\"]\n",
    "x_test_small = vectorizer_small.transform(list(test[\"review\"]))\n",
    "y_test_small = test[\"rating\"]\n",
    "light_model = LogisticRegression()\n",
    "light_model.fit(x_small, y_small)\n",
    "proba = light_model.predict_proba(x_test_small)\n",
    "print(\"most positive: \", test.iloc[proba.T[1].argmax()][\"review\"])\n",
    "print(\"\\nmost negative: \", test.iloc[proba.T[0].argmax()][\"review\"])\n",
    "light_model.score(x_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love: 1.3989926093455205\n",
      "great: 0.956031493402818\n",
      "easy: 1.1842153716159887\n",
      "old: 0.08116863112747041\n",
      "little: 0.500933979332243\n",
      "perfect: 1.4838465890012005\n",
      "loves: 1.7282504597783226\n",
      "well: 0.5018156386229154\n",
      "able: 0.20141386562319\n",
      "car: 0.07325059002318328\n",
      "broke: -1.6816034616166022\n",
      "less: -0.21324781686452343\n",
      "even: -0.5186847771872509\n",
      "waste: -1.9527997877757763\n",
      "disappointed: -2.3661271204599887\n",
      "work: -0.6450621082926463\n",
      "product: -0.31786006654357496\n",
      "money: -0.9293136885357741\n",
      "would: -0.3471683883756012\n",
      "return: -2.0392974055141204\n"
     ]
    }
   ],
   "source": [
    "#b)\n",
    "for word, coef in zip(vectorizer_small.get_feature_names_out(), light_model.coef_[0]):\n",
    "    print(f\"{word}: {coef}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Once more, we instruct our program to identify relevant words, but this time we employ a specific dictionary that we have provided. Given that the number of words in the list is less than the total count, we anticipate a potential decrease in prediction accuracy. Additionally, we evaluate the strength or weight assigned to each word at the conclusion of the process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light model - 0.8675302089892357\n",
      "Standard model - 0.9303769002428712\n"
     ]
    }
   ],
   "source": [
    "#c)\n",
    "print(f\"Light model - {light_model.score(x_test_small, y_test_small)}\")\n",
    "print(f\"Standard model - {model.score(x_test, y_test)}\")\n",
    "\n",
    "#hint: %time, %timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985 µs ± 13.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "CPU times: total: 3.28 s\n",
      "Wall time: 8.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%timeit\n",
    "light_model.predict(x_test_small)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.54 ms ± 710 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "CPU times: total: 3.16 s\n",
      "Wall time: 7.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%timeit\n",
    "model.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing our light model with original model we can observe that original model has around 6 % points better accuracy but with the cost of being around 9 times slower comparing to lighter model."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
